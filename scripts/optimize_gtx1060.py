#!/usr/bin/env python3
"""
GTX 1060 optimization configuration script.
Automatically configures optimal settings for GTX 1060 6GB performance.
"""
import os
import json
import logging
from pathlib import Path
import argparse

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class GTX1060Optimizer:
    """Optimizer for GTX 1060 6GB configuration."""
    
    def __init__(self):
        self.config_dir = Path(".kiro/settings") if Path(".kiro").exists() else Path("config")
        self.env_file = Path(".env")
        
    def optimize_environment_variables(self):
        """Set optimal environment variables for GTX 1060."""
        logger.info("Optimizing environment variables for GTX 1060")
        
        optimal_env_vars = {
            # CUDA optimizations
            "CUDA_VISIBLE_DEVICES": "0",
            "CUDA_LAUNCH_BLOCKING": "0",
            "CUDA_CACHE_DISABLE": "0",
            
            # Memory management
            "PYTORCH_CUDA_ALLOC_CONF": "max_split_size_mb:512,garbage_collection_threshold:0.6",
            
            # Model settings
            "MODEL_CACHE_DIR": "./models",
            "MODEL_TORCH_DTYPE": "float16",
            "MODEL_ENABLE_ATTENTION_SLICING": "true",
            "MODEL_ENABLE_CPU_OFFLOAD": "true",
            "MODEL_ENABLE_VAE_SLICING": "true",
            "MODEL_SAFETY_CHECKER": "false",  # Disable to save memory
            
            # GPU settings
            "GPU_MEMORY_FRACTION": "0.85",  # Use 85% of 6GB = ~5.1GB
            "GPU_ALLOW_GROWTH": "true",
            
            # API settings
            "API_MAX_CONCURRENT_REQUESTS": "1",  # Single request for GTX 1060
            "API_GENERATION_TIMEOUT_SECONDS": "90",
            
            # Resource limits
            "RESOURCE_MEMORY_LIMIT": "8G",
            "RESOURCE_CPU_LIMIT": "4.0",
            
            # Logging
            "LOG_LEVEL": "INFO"
        }
        
        # Read existing .env file if it exists
        existing_vars = {}
        if self.env_file.exists():
            with open(self.env_file, 'r') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#') and '=' in line:
                        key, value = line.split('=', 1)
                        existing_vars[key] = value
        
        # Merge with optimal settings (optimal settings take precedence)
        final_vars = {**existing_vars, **optimal_env_vars}
        
        # Write optimized .env file
        with open(self.env_file, 'w') as f:
            f.write("# GTX 1060 Optimized Configuration\n")
            f.write("# Generated by optimize_gtx1060.py\n\n")
            
            for key, value in final_vars.items():
                f.write(f"{key}={value}\n")
        
        logger.info(f"Environment variables optimized and saved to {self.env_file}")
        
    def create_docker_compose_override(self):
        """Create docker-compose override for GTX 1060 optimization."""
        logger.info("Creating Docker Compose override for GTX 1060")
        
        override_config = {
            "version": "3.8",
            "services": {
                "stable-diffusion-api": {
                    "deploy": {
                        "resources": {
                            "limits": {
                                "memory": "8G",
                                "cpus": "4.0"
                            },
                            "reservations": {
                                "devices": [
                                    {
                                        "driver": "nvidia",
                                        "count": 1,
                                        "capabilities": ["gpu"]
                                    }
                                ]
                            }
                        }
                    },
                    "environment": {
                        "NVIDIA_VISIBLE_DEVICES": "0",
                        "NVIDIA_DRIVER_CAPABILITIES": "compute,utility"
                    },
                    "shm_size": "2g"  # Increased shared memory for model loading
                }
            }
        }
        
        override_file = Path("docker-compose.gtx1060.yml")
        with open(override_file, 'w') as f:
            import yaml
            yaml.dump(override_config, f, default_flow_style=False)
        
        logger.info(f"Docker Compose override created: {override_file}")
        logger.info("Use with: docker-compose -f docker-compose.yml -f docker-compose.gtx1060.yml up")
        
    def create_benchmark_config(self):
        """Create benchmark configuration optimized for GTX 1060."""
        logger.info("Creating benchmark configuration for GTX 1060")
        
        benchmark_config = {
            "test_scenarios": [
                {"width": 512, "height": 512, "steps": 20, "description": "Standard quality"},
                {"width": 512, "height": 512, "steps": 30, "description": "High quality"},
                {"width": 768, "768": 768, "steps": 20, "description": "Large image"},
                {"width": 256, "height": 256, "steps": 20, "description": "Fast generation"}
            ],
            "memory_limits": {
                "max_vram_gb": 5.0,
                "safety_margin_gb": 0.5,
                "queue_size": 5
            },
            "performance_targets": {
                "max_generation_time_512": 45,  # seconds
                "max_generation_time_768": 90,  # seconds
                "min_success_rate": 95,  # percent
                "max_queue_wait_time": 30  # seconds
            }
        }
        
        config_file = Path("gtx1060_benchmark_config.json")
        with open(config_file, 'w') as f:
            json.dump(benchmark_config, f, indent=2)
        
        logger.info(f"Benchmark configuration created: {config_file}")
        
    def create_monitoring_config(self):
        """Create monitoring configuration for GTX 1060."""
        logger.info("Creating monitoring configuration")
        
        monitoring_config = {
            "memory_monitoring": {
                "max_vram_usage_gb": 5.0,
                "check_interval_seconds": 5,
                "alert_threshold_percent": 80,
                "cleanup_threshold_percent": 90
            },
            "performance_monitoring": {
                "metrics_retention_hours": 24,
                "export_interval_minutes": 60,
                "alert_thresholds": {
                    "max_generation_time": 120,
                    "max_error_rate_percent": 10,
                    "max_queue_wait_time": 60
                }
            },
            "system_monitoring": {
                "cpu_alert_threshold": 90,
                "ram_alert_threshold": 85,
                "gpu_temp_alert_threshold": 80
            }
        }
        
        config_file = Path("gtx1060_monitoring_config.json")
        with open(config_file, 'w') as f:
            json.dump(monitoring_config, f, indent=2)
        
        logger.info(f"Monitoring configuration created: {config_file}")
        
    def create_optimization_guide(self):
        """Create optimization guide documentation."""
        logger.info("Creating GTX 1060 optimization guide")
        
        guide_content = """# GTX 1060 6GB Optimization Guide

## Overview
This guide provides optimization recommendations for running Stable Diffusion API on GTX 1060 6GB.

## Memory Management
- **VRAM Limit**: 5.0GB (leaving 1GB safety margin)
- **Attention Slicing**: Enabled (reduces memory usage by ~30%)
- **CPU Offloading**: Enabled (moves unused components to CPU)
- **VAE Slicing**: Enabled (reduces VAE memory usage)
- **Safety Checker**: Disabled (saves ~500MB VRAM)

## Recommended Settings

### Image Generation
- **Max Resolution**: 768x768 (higher may cause OOM)
- **Optimal Resolution**: 512x512
- **Steps**: 20-30 (good quality/speed balance)
- **Concurrent Requests**: 1 (GTX 1060 can't handle multiple)

### Performance Targets
- **512x512, 20 steps**: ~25-35 seconds
- **512x512, 30 steps**: ~35-45 seconds
- **768x768, 20 steps**: ~60-90 seconds

### Memory Usage
- **Model Loading**: ~3.5GB VRAM
- **512x512 Generation**: ~2.5GB additional
- **768x768 Generation**: ~4.0GB additional

## Troubleshooting

### Out of Memory Errors
1. Reduce image resolution
2. Reduce number of steps
3. Enable all memory optimizations
4. Restart the service to clear memory leaks

### Slow Generation
1. Check GPU temperature (should be <80Â°C)
2. Ensure CUDA drivers are up to date
3. Monitor system RAM usage
4. Check for background processes using GPU

### Queue Issues
1. Reduce concurrent request limit to 1
2. Increase request timeout
3. Monitor queue status via API

## Monitoring Commands

### Check GPU Status
```bash
nvidia-smi
```

### Monitor API Performance
```bash
curl http://localhost:8000/performance/stats
curl http://localhost:8000/performance/queue
```

### Run Benchmarks
```bash
python scripts/benchmark_performance.py
python scripts/memory_stress_test.py
```

## Configuration Files

### Environment Variables (.env)
```
MODEL_TORCH_DTYPE=float16
MODEL_ENABLE_ATTENTION_SLICING=true
MODEL_ENABLE_CPU_OFFLOAD=true
GPU_MEMORY_FRACTION=0.85
API_MAX_CONCURRENT_REQUESTS=1
```

### Docker Compose
Use the GTX 1060 specific override:
```bash
docker-compose -f docker-compose.yml -f docker-compose.gtx1060.yml up
```

## Performance Optimization Tips

1. **Keep model loaded**: Avoid restarting the service frequently
2. **Monitor memory**: Use performance endpoints to track usage
3. **Batch similar requests**: Group requests with similar parameters
4. **Use appropriate timeouts**: Set realistic timeouts based on image size
5. **Regular cleanup**: Restart service periodically to clear memory leaks

## Hardware Recommendations

- **System RAM**: 16GB+ recommended
- **CPU**: 4+ cores recommended
- **Storage**: SSD for model storage
- **Cooling**: Ensure adequate GPU cooling
"""
        
        guide_file = Path("GTX1060_OPTIMIZATION_GUIDE.md")
        with open(guide_file, 'w') as f:
            f.write(guide_content)
        
        logger.info(f"Optimization guide created: {guide_file}")
        
    def run_optimization(self, skip_docker=False):
        """Run complete optimization process."""
        logger.info("Starting GTX 1060 optimization process")
        
        try:
            # Create necessary directories
            self.config_dir.mkdir(parents=True, exist_ok=True)
            
            # Run optimization steps
            self.optimize_environment_variables()
            
            if not skip_docker:
                try:
                    self.create_docker_compose_override()
                except ImportError:
                    logger.warning("PyYAML not available, skipping Docker Compose override")
            
            self.create_benchmark_config()
            self.create_monitoring_config()
            self.create_optimization_guide()
            
            logger.info("GTX 1060 optimization completed successfully!")
            logger.info("Next steps:")
            logger.info("1. Review the generated .env file")
            logger.info("2. Read GTX1060_OPTIMIZATION_GUIDE.md")
            logger.info("3. Run benchmarks to validate performance")
            logger.info("4. Start the API service")
            
        except Exception as e:
            logger.error(f"Optimization failed: {e}")
            raise


def main():
    """Main optimization script."""
    parser = argparse.ArgumentParser(description="GTX 1060 Optimization Script")
    parser.add_argument("--skip-docker", action="store_true", help="Skip Docker Compose override creation")
    parser.add_argument("--env-only", action="store_true", help="Only optimize environment variables")
    
    args = parser.parse_args()
    
    optimizer = GTX1060Optimizer()
    
    try:
        if args.env_only:
            optimizer.optimize_environment_variables()
        else:
            optimizer.run_optimization(skip_docker=args.skip_docker)
        
        return 0
        
    except Exception as e:
        logger.error(f"Optimization script failed: {e}")
        return 1


if __name__ == "__main__":
    exit(main())